{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DecisionTrees3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNDEZNVg1xi5RqjfqJg90bZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/DecisionTrees3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfsMr6DduHNq"
      },
      "source": [
        "!git clone -l -s https://github.com/cagBRT/Machine-Learning.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9sYJ8eCjIfw"
      },
      "source": [
        "**Decision Tree Classifier on the Diabetes Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylAON1s9j2TR"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK1RRyr5jVjh"
      },
      "source": [
        "col_names = ['pregnancies', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
        "# load dataset\n",
        "diabetes= pd.read_csv(\"pima_indians_diabetes.csv\", header=None, names=col_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bu-bgSckE7a"
      },
      "source": [
        "diabetes = diabetes.drop([0])\n",
        "diabetes.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh2ce8LJkP4-"
      },
      "source": [
        "#split dataset in features and target variable\n",
        "feature_cols = ['pregnancies', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
        "X = diabetes[feature_cols] # Features\n",
        "y = diabetes.label # Target variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7qQHLj3kXIr"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E-jSDs3kaW-"
      },
      "source": [
        "# Create Decision Tree classifer object\n",
        "diabetes_clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "diabetes_clf = diabetes_clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = diabetes_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01KT3ZU8lWTZ"
      },
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXNuh1hblagP"
      },
      "source": [
        "import pydotplus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ0MNk2emEVa"
      },
      "source": [
        "!pip install six"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-PbkzFbIPyF"
      },
      "source": [
        "In the decision tree chart, each internal node has a decision rule that splits the data. Gini referred as Gini ratio, which measures the impurity of the node. You can say a node is pure when all of its records belong to the same class, such nodes known as the leaf node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vXZ1azBIS8l"
      },
      "source": [
        "Here, the resultant tree is unpruned. This unpruned tree is unexplainable and not easy to understand. In the next section, let's optimize it by pruning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkS4aK3ovHc8"
      },
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(diabetes_clf, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "graph.write_png('diabetes.png')\n",
        "Image(graph.create_png())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D39pm18BlXn7"
      },
      "source": [
        "**Can we improve the accuracy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46PfZ9U5IYjq"
      },
      "source": [
        "Optimizing Decision Tree Performance\n",
        "criterion : optional (default=”gini”) or Choose attribute selection measure: This parameter allows us to use the different-different attribute selection measure. Supported criteria are “gini” for the Gini index and “entropy” for the information gain.\n",
        "\n",
        "splitter : string, optional (default=”best”) or Split Strategy: This parameter allows us to choose the split strategy. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
        "\n",
        "max_depth : int or None, optional (default=None) or Maximum Depth of a Tree: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAY-emhOIqVp"
      },
      "source": [
        "This pruned model is less complex, explainable, and easy to understand "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wra5t0PmvK5"
      },
      "source": [
        "# Create Decision Tree classifer object\n",
        "better_diabetes_clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "better_diabetes_clf = better_diabetes_clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred_better = better_diabetes_clf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_better))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9dwbInjnHzx"
      },
      "source": [
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "dot_data = StringIO()\n",
        "export_graphviz(better_diabetes_clf, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True, feature_names = feature_cols,class_names=['0','1'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "graph.write_png('bettter_diabetes.png')\n",
        "Image(graph.create_png())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctxqYFwZI0uT"
      },
      "source": [
        "**Pros**<br>\n",
        "- Decision trees are easy to interpret and visualize.\n",
        "- It can easily capture Non-linear patterns.\n",
        "It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.<br>\n",
        "- It can be used for feature engineering such as predicting missing values, suitable for variable selection.<br>\n",
        "- The decision tree has no assumptions about distribution because of the non-parametric nature of the algorithm. (Source)<br>\n",
        "\n",
        "**Cons** <br>\n",
        "- Sensitive to noisy data. It can overfit noisy data.<br>\n",
        "- The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.<br>\n",
        "- Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.<br>"
      ]
    }
  ]
}