{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "boston_housing.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNTb5exwBEwtXL/WXMN4Ap4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/HP3%20boston_housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEgIWxQuOSri"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/Machine-Learning.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0VCv_jxQ_l"
      },
      "source": [
        "# **Can the median value (medv) Boston housing market be predicted from the given feature set?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWtK6DJgwqS4"
      },
      "source": [
        "# **The dataset definitions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x25DecR-OUC1"
      },
      "source": [
        "CRIM: This column represents per capita crime rate by town<br>\n",
        "ZN: This column represents the proportion of residential land zoned for lots larger than 25,000 sq.ft.<br>\n",
        "INDUS: This column represents the proportion of non-retail business acres per town.<br>\n",
        "CHAS: This column represents the Charles River dummy variable (this is equal to 1 if tract bounds river; 0 otherwise)<br>\n",
        "NOX: This column represents the concentration of the nitric oxide (parts per 10 million)<br>\n",
        "RM: This column represents the average number of rooms per dwelling<br>\n",
        "AGE: This column represents the proportion of owner-occupied units built prior to 1940<br>\n",
        "DIS: This column represents the weighted distances to five Boston employment centers<br>\n",
        "RAD: This column represents the index of accessibility to radial highways<br>\n",
        "TAX: This column represents the full-value property-tax rate per \\$10,000 <br>\n",
        "PTRATIO: This column represents the pupil-teacher ratio by town <br>\n",
        "B: This is calculated as 1000(Bk — 0.63)², where Bk is the proportion of people of African American descent by town<br>\n",
        "LSTAT: This is the percentage lower status of the population<br>\n",
        "MEDV: This is the median value of owner-occupied homes in $1000s<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7u0Rqnewu29"
      },
      "source": [
        "# **Load the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbicIlT6PQ9J"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnwkfHfxw0Y2"
      },
      "source": [
        "# **Get the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7aRouidPWSj"
      },
      "source": [
        "boston_housing_data = pd.read_csv(\"boston_housing.csv\", sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGXpHI3VPaI-"
      },
      "source": [
        "boston_housing_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUbgLpx8tua0"
      },
      "source": [
        "boston_housing_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0n4rhYdSLVd"
      },
      "source": [
        "Sometimes model performance can be improved by grouping (or binning) the data. \n",
        "<br>\n",
        "Binning methods smooth a sorted data value by consulting its “neighborhood”, that is, the values around it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sttw92mGSzRC"
      },
      "source": [
        "**Uncomment the code below to use binning on the column.** <br>\n",
        "You can modify the code and use it on other columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s6YxDUyPlkW"
      },
      "source": [
        "#boston_housing_data['age_bin_round'] = np.array(np.floor(\n",
        "#                              np.array(boston_housing_data['age']) / 10.))\n",
        "#boston_housing_data[['age_bin_round']]\n",
        "#boston_housing_data.drop(columns=['age'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj9QOqives_U"
      },
      "source": [
        "Did binning add bias to our data?<br>\n",
        "Print out the counts for each bin and see what the breakdown of the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9elPNxqesHf"
      },
      "source": [
        "#boston_housing_data['age_bin_round'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6PpDLKcw4U6"
      },
      "source": [
        "# **Data Correlation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTqRHefEwhDH"
      },
      "source": [
        "corr = boston_housing_data.corr()\n",
        "sns.heatmap(corr, \n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vItcoFKyz6g4"
      },
      "source": [
        "# **Tune the train-test split**\n",
        "Select a split frac = 0.5 to 0.95"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrWchrJLt33i"
      },
      "source": [
        "train_dataset = boston_housing_data.sample(frac=0.5,random_state=0)\n",
        "test_dataset = boston_housing_data.drop(train_dataset.index)\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdgiTbELuDEZ"
      },
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop(\"medv\")\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrH-y-nXuJxq"
      },
      "source": [
        "test_stats = test_dataset.describe()\n",
        "test_stats.pop(\"medv\")\n",
        "test_stats = test_stats.transpose()\n",
        "test_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPjS_Dz0uMx6"
      },
      "source": [
        "train_labels = train_dataset.pop('medv')\n",
        "test_labels = test_dataset.pop('medv')\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L10Zo51Dw_e8"
      },
      "source": [
        "# **Normalize the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxtb8DXWuTx9"
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbjLowGOxD7t"
      },
      "source": [
        "# **The model**\n",
        "Modify the model<br>\n",
        "Change the number of neurons per layer<br>\n",
        "Change the numbrer of layers<br>\n",
        "Change the activation function:<br>\n",
        ">sigmoid<br>\n",
        "tanh<br>\n",
        "relu<br>\n",
        "leaky_relu<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEz3YQjDuYXb"
      },
      "source": [
        "inputs = len(train_dataset.keys())\n",
        "print(\"number of inputs to the model = \" + str(inputs))\n",
        "\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(8, activation=tf.nn.sigmoid,input_shape=([len(train_dataset.keys())]),),\n",
        "    #layers.Dropout(0.2),\n",
        "    #layers.Dense(8, activation=tf.nn.sigmoid),\n",
        "    layers.Dense(8, activation=tf.nn.sigmoid),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "  return model\n",
        "  print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWWLyx8suepJ"
      },
      "source": [
        "model = build_model()\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAI31wsWxIQU"
      },
      "source": [
        "# **Train the model**<br>\n",
        "Usually, you would want to change the number of epochs but in this case, we are using early stopping. \n",
        "<br><br>\n",
        "Tune the batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mBi4HLvuhb6"
      },
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "\n",
        "model = build_model()\n",
        "EPOCHS = 1000\n",
        "\n",
        "# The patience parameter is the amount of epochs to check for improvement\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "#Tune the batch_size\n",
        "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,batch_size=2,\n",
        "                    validation_split = 0.2, verbose=0, callbacks=[early_stop])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd_ZnTCFusO7"
      },
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error [MPG]')\n",
        "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,5])\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Square Error [$MPG^2$]')\n",
        "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek_255KLuwyS"
      },
      "source": [
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=1)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} medv\".format(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAAUSVWlu2xF"
      },
      "source": [
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [wage]')\n",
        "plt.ylabel('Predictions [wage]')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0,plt.xlim()[1]])\n",
        "plt.ylim([0,plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_AGOfmuu56H"
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins = 25)\n",
        "plt.xlabel(\"Prediction Error [medv]\")\n",
        "_ = plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}