{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN/mqtpcaQHG5sgft3bFyM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/KNN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkBoQWp8oc3P"
      },
      "source": [
        "!git clone -l -s https://github.com/cagBRT/Machine-Learning.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYKsNmXGot35"
      },
      "source": [
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDOWgBKbnP5N"
      },
      "source": [
        "K-nearest neighbors (KNN) algorithm uses ‘feature similarity’ to predict the values of new datapoints which means that the new data point will be assigned a value based on how closely it matches the points in the training set. \n",
        "\n",
        "Step 1 − For implementing any algorithm, we need dataset. So during the first step of KNN, we must load the training as well as test data.\n",
        "\n",
        "Step 2 − Next, we need to choose the value of K i.e. the nearest data points. K can be any integer.\n",
        "\n",
        "Step 3 − For each point in the test data do the following −\n",
        "\n",
        ">Calculate the distance between test data and each row of training data with the help of any method : \n",
        "- Euclidean \n",
        "- Manhattan \n",
        "- Hamming distance<br>\n",
        "\n",
        "> The most commonly used method to calculate distance is Euclidean.\n",
        "\n",
        ">Based on the distance value, sort them in ascending order.\n",
        "\n",
        ">Choose the top K rows from the sorted array.\n",
        "\n",
        ">Assign a class to the test point based on most frequent class of these rows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MY9l6yrpHqI"
      },
      "source": [
        "For example: \n",
        "This is our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8TMyJ3Poa_N"
      },
      "source": [
        "Image(\"images/concept_of_knn.jpg\" , width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH_XXxmVpMCY"
      },
      "source": [
        "A new data point, the black dot, is added. To which class should it be added?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Immez--pb1G"
      },
      "source": [
        "Assuming K = 3 i.e. it would find three nearest data points.<br>\n",
        "We can see in the diagram the three nearest neighbors of the data point with black dot. Among those three, two of them lies in Red class , so the black dot will be assigned to the red class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yplHt8-3o_El"
      },
      "source": [
        "Image(\"images/knn_algorithm.jpg\" , width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMXcg2eJqAHW"
      },
      "source": [
        "**Pros**<br>\n",
        "It is very simple algorithm to understand and interpret.<br>\n",
        "\n",
        "It is very useful for nonlinear data because there is no assumption about data in this algorithm.<br>\n",
        "\n",
        "It is a versatile algorithm as we can use it for classification as well as regression.<br>\n",
        "\n",
        "It has relatively high accuracy but there are much better supervised learning models than KNN.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CCsGkaSqCyH"
      },
      "source": [
        "**Cons**<br>\n",
        "It is computationally a bit expensive algorithm because it stores all the training data.<br>\n",
        "\n",
        "High memory storage required as compared to other supervised learning algorithms.<br>\n",
        "\n",
        "Prediction is slow in case of big N.<br>\n",
        "\n",
        "It is very sensitive to the scale of data as well as irrelevant features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWcVy--1qUQz"
      },
      "source": [
        "**Examples of KNN usage**: <br>\n",
        "Banking System<br>\n",
        ">KNN can be used in banking system to predict weather an individual is fit for loan approval. Does that individual have the characteristics similar to the defaulters one?\n",
        "\n",
        "Calculating Credit Ratings<br>\n",
        ">KNN algorithms can be used to find an individual’s credit rating by comparing with the persons having similar traits.\n",
        "\n",
        "Politics<br>\n",
        ">With the help of KNN algorithms, we can classify a potential voter into various classes like “Will Vote”, “Will not Vote”, “Will Vote to Party ‘Congress’, “Will Vote to Party ‘BJP’.\n",
        "\n",
        "Other areas in which KNN algorithm can be used:<<br>\n",
        ">Speech Recognition, Handwriting Detection, Image Recognition and Video Recognition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ymuUvgwdqc"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JujwqVUtVNoh"
      },
      "source": [
        "Create some data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLThYzrVQMr"
      },
      "source": [
        "X = [[0], [1], [2], [3], [5],[4], [-1],[-2],[-3] ]\n",
        "y = [0, 0, 1, 1, 1, 1, 0, 0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6tUV7LBVRP8"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "fig = plt.scatter(X,y, marker='o', c=X,s=25)\n",
        "plt.colorbar(fig)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv84EtDlWGkV"
      },
      "source": [
        "Create a model and train it on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xEvdZXgWL5G"
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE8uRrR2WPGW"
      },
      "source": [
        "Make predictions<br>\n",
        "\n",
        "The predict_proba() returns the number of votes for each class, divided by the number of trees in the forest. Your precision is exactly 1/n_estimators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXJC9u-dwTEs"
      },
      "source": [
        "testp = 2.4\n",
        "print(neigh.predict([[testp]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxPz4ZIxWWQa"
      },
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "fig = plt.scatter(X,y, marker='o', c=X,s=25)\n",
        "plt.plot(testp,neigh.predict([[testp]]),'r+' )\n",
        "plt.colorbar(fig)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFs_XqvPq5mk"
      },
      "source": [
        "# **Assignment**<br>\n",
        "Add more data to the dataset. <br>\n",
        "Run the model. <br>\n",
        "Share with the class what you have learned. <br>"
      ]
    }
  ]
}