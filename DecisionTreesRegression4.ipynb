{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DecisionTreesRegression4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZuXQuBDNnBj7fW7ZDGJFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/DecisionTreesRegression4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXfyI6Adf-1O"
      },
      "source": [
        "In this notebool a decision tree is boosted using the AdaBoost.R2 algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9II8XkLgJ9W"
      },
      "source": [
        "AdaBoost (Adaptive Boosting) can be used to improve (or boost) the performance of any ML algorithm. <br>\n",
        "The biggest boost comes when using it with any model where accuracy is slightly better than random chance on a classification model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w8cq2vBgtiM"
      },
      "source": [
        "The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8k_-nh8hlTO"
      },
      "source": [
        "**Import the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxKb5zDkwRjy"
      },
      "source": [
        "# importing necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hTjiYg4hn3c"
      },
      "source": [
        "**Create a dataset with random noise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVW_TWb_wVNX"
      },
      "source": [
        "# Create the dataset\n",
        "rng = np.random.RandomState(1)\n",
        "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
        "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zfc11EcyD34"
      },
      "source": [
        "plt.plot(X,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQWxqQjWhtI9"
      },
      "source": [
        "**Split the data into training and test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2U6Hl0JUbPq"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOXeoVQLwySr"
      },
      "source": [
        "print(\"X_train shape:\",X_train.shape)\n",
        "print(\"y_train shape:\",y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGDYR3j4hz96"
      },
      "source": [
        "**Create and train two models:**<br>\n",
        "1. Decision tree rergressor\n",
        "2. AdaBoost regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f07CeWDMwYDU"
      },
      "source": [
        "# Fit regression model\n",
        "regr_1 = DecisionTreeRegressor(max_depth=1)\n",
        "#notice the depth is 4 for the AdaBoostRegressor\n",
        "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
        "                           n_estimators=300, random_state=rng)\n",
        "\n",
        "regr_1.fit(X_train, y_train)\n",
        "regr_2.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxsB22nciWyw"
      },
      "source": [
        "**Predict with the test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn9thkDhwb_V"
      },
      "source": [
        "# Predict\n",
        "y_1 = regr_1.predict(X_train)\n",
        "y_2 = regr_2.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iIviS1QexC8"
      },
      "source": [
        "print(y_1.shape)\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihyYb3NDihJF"
      },
      "source": [
        "**Compare the accuracy scores of the two models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWiLjpvDT42V"
      },
      "source": [
        "print(regr_1.score(X_test,y_test))\n",
        "print(regr_2.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rip9-zeHim_Y"
      },
      "source": [
        "**Predict using the entire dataset and plot the full dataset predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYhLYeULfeER"
      },
      "source": [
        "y_1 = regr_1.predict(X)\n",
        "y_2 = regr_2.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88BkmEo9wNlk"
      },
      "source": [
        "# Plot the results\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(X, y, c=\"k\", label=\"training samples\")\n",
        "plt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
        "plt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
        "plt.xlabel(\"data\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.title(\"Boosted Decision Tree Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB5vKTKihEl9"
      },
      "source": [
        "**Assignment**<br>\n",
        "1. Change the depth of the decision tree. <br>\n",
        "Note the difference in the performance \n",
        "2. Change the number of esstimators. What kind of impact doess this have on the performance of the models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCyV3RfOvllf"
      },
      "source": [
        "# **Multi-class AdaBoosted Decision Trees**\n",
        "\n",
        "https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html?highlight=decision%20trees\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "webfQvdev6C7"
      },
      "source": [
        "# Author: Noel Dawe <noel.dawe@gmail.com>\n",
        "#\n",
        "# License: BSD 3 clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_gaussian_quantiles\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WEmpLoovfoc"
      },
      "source": [
        "# Author: Noel Dawe <noel.dawe@gmail.com>\n",
        "#\n",
        "# License: BSD 3 clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_gaussian_quantiles\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "X, y = make_gaussian_quantiles(n_samples=13000, n_features=10,\n",
        "                               n_classes=3, random_state=1)\n",
        "\n",
        "n_split = 3000\n",
        "\n",
        "X_train, X_test = X[:n_split], X[n_split:]\n",
        "y_train, y_test = y[:n_split], y[n_split:]\n",
        "\n",
        "bdt_real = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=2),\n",
        "    n_estimators=600,\n",
        "    learning_rate=1)\n",
        "\n",
        "bdt_discrete = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=2),\n",
        "    n_estimators=600,\n",
        "    learning_rate=1.5,\n",
        "    algorithm=\"SAMME\")\n",
        "\n",
        "bdt_real.fit(X_train, y_train)\n",
        "bdt_discrete.fit(X_train, y_train)\n",
        "\n",
        "real_test_errors = []\n",
        "discrete_test_errors = []\n",
        "\n",
        "for real_test_predict, discrete_train_predict in zip(\n",
        "        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):\n",
        "    real_test_errors.append(\n",
        "        1. - accuracy_score(real_test_predict, y_test))\n",
        "    discrete_test_errors.append(\n",
        "        1. - accuracy_score(discrete_train_predict, y_test))\n",
        "\n",
        "n_trees_discrete = len(bdt_discrete)\n",
        "n_trees_real = len(bdt_real)\n",
        "\n",
        "# Boosting might terminate early, but the following arrays are always\n",
        "# n_estimators long. We crop them to the actual number of trees here:\n",
        "discrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\n",
        "real_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\n",
        "discrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.plot(range(1, n_trees_discrete + 1),\n",
        "         discrete_test_errors, c='black', label='SAMME')\n",
        "plt.plot(range(1, n_trees_real + 1),\n",
        "         real_test_errors, c='black',\n",
        "         linestyle='dashed', label='SAMME.R')\n",
        "plt.legend()\n",
        "plt.ylim(0.18, 0.62)\n",
        "plt.ylabel('Test Error')\n",
        "plt.xlabel('Number of Trees')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,\n",
        "         \"b\", label='SAMME', alpha=.5)\n",
        "plt.plot(range(1, n_trees_real + 1), real_estimator_errors,\n",
        "         \"r\", label='SAMME.R', alpha=.5)\n",
        "plt.legend()\n",
        "plt.ylabel('Error')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylim((.2,\n",
        "         max(real_estimator_errors.max(),\n",
        "             discrete_estimator_errors.max()) * 1.2))\n",
        "plt.xlim((-20, len(bdt_discrete) + 20))\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,\n",
        "         \"b\", label='SAMME')\n",
        "plt.legend()\n",
        "plt.ylabel('Weight')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylim((0, discrete_estimator_weights.max() * 1.2))\n",
        "plt.xlim((-20, n_trees_discrete + 20))\n",
        "\n",
        "# prevent overlapping y-axis labels\n",
        "plt.subplots_adjust(wspace=0.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}