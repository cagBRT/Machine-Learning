{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrossValidation2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPsWG7FOQKu569MbA9IIjem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/CrossValidation2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4JYUxqLfD-T"
      },
      "source": [
        "Let's compare cross validation on different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Xu51exfr8z"
      },
      "source": [
        "from numpy import mean\n",
        "from numpy import isnan\n",
        "from numpy import asarray\n",
        "from numpy import polyfit\n",
        "from scipy.stats import pearsonr\n",
        "from matplotlib import pyplot\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogi_qkRJf9qd"
      },
      "source": [
        "Create a synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xffxKVpcf7Yd"
      },
      "source": [
        "# create the dataset\n",
        "#For the assignment, \n",
        "#Change the dataset here\n",
        "def get_dataset(n_samples=100):\n",
        "\tX, y = make_classification(n_samples=n_samples, n_features=20, \n",
        "                            n_informative=15, n_redundant=5, random_state=1)\n",
        "\treturn X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WuaiKoUe8mF"
      },
      "source": [
        "#create a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(LogisticRegression())\n",
        "\tmodels.append(RidgeClassifier())\n",
        "\tmodels.append(SGDClassifier())\n",
        "\tmodels.append(PassiveAggressiveClassifier())\n",
        "\tmodels.append(KNeighborsClassifier())\n",
        "\tmodels.append(DecisionTreeClassifier())\n",
        "\tmodels.append(ExtraTreeClassifier())\n",
        "\tmodels.append(LinearSVC())\n",
        "\tmodels.append(SVC())\n",
        "\tmodels.append(GaussianNB())\n",
        "\tmodels.append(AdaBoostClassifier())\n",
        "\tmodels.append(BaggingClassifier())\n",
        "\tmodels.append(RandomForestClassifier())\n",
        "\tmodels.append(ExtraTreesClassifier())\n",
        "\tmodels.append(GaussianProcessClassifier())\n",
        "\tmodels.append(GradientBoostingClassifier())\n",
        "\tmodels.append(LinearDiscriminantAnalysis())\n",
        "\tmodels.append(QuadraticDiscriminantAnalysis())\n",
        "\treturn models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojEtOE-VgWQN"
      },
      "source": [
        "# evaluate the model using a given test condition\n",
        "def evaluate_model(cv, model):\n",
        "\t# get the dataset\n",
        "\tX, y = get_dataset()\n",
        "\t# evaluate the model\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\t# return scores\n",
        "\treturn mean(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uCPTeRrfWCm"
      },
      "source": [
        "We will use k=10 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZz39e2SfVg6"
      },
      "source": [
        "# define test conditions\n",
        "ideal_cv = LeaveOneOut()\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# get the list of models to consider\n",
        "models = get_models()\n",
        "# collect results\n",
        "ideal_results, cv_results = list(), list()\n",
        "# evaluate each model\n",
        "for model in models:\n",
        "\t# evaluate model using each test condition\n",
        "\tcv_mean = evaluate_model(cv, model)\n",
        "\tideal_mean = evaluate_model(ideal_cv, model)\n",
        "\t# check for invalid results\n",
        "\tif isnan(cv_mean) or isnan(ideal_mean):\n",
        "\t\tcontinue\n",
        "\t# store results\n",
        "\tcv_results.append(cv_mean)\n",
        "\tideal_results.append(ideal_mean)\n",
        "\t# summarize progress\n",
        "\tprint('>%s: ideal=%.3f, cv=%.3f' % (type(model).__name__, ideal_mean, cv_mean))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlYDeUXugdRV"
      },
      "source": [
        "# define test conditions\n",
        "ideal_cv = LeaveOneOut()\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# get the list of models to consider\n",
        "models = get_models()\n",
        "# collect results\n",
        "ideal_results, cv_results = list(), list()\n",
        "# evaluate each model\n",
        "for model in models:\n",
        "\t# evaluate model using each test condition\n",
        "\tcv_mean = evaluate_model(cv, model)\n",
        "\tideal_mean = evaluate_model(ideal_cv, model)\n",
        "\t# check for invalid results\n",
        "\tif isnan(cv_mean) or isnan(ideal_mean):\n",
        "\t\tcontinue\n",
        "\t# store results\n",
        "\tcv_results.append(cv_mean)\n",
        "\tideal_results.append(ideal_mean)\n",
        "\t# summarize progress\n",
        "\tprint('>%s: ideal=%.3f, cv=%.3f' % (type(model).__name__, ideal_mean, cv_mean))\n",
        "# calculate the correlation between each test condition\n",
        "corr, _ = pearsonr(cv_results, ideal_results)\n",
        "print('Correlation: %.3f' % corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnn0E1t9g62b"
      },
      "source": [
        "We can see that the correlation between the two sets.<bt>\n",
        "In this case, a high number is reported, which is a good strong positive correlation. The results suggest that 10-fold cross-validation does provide a good approximation for the LOOCV test on this dataset as calculated with 18 popular machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM--zztxghT4"
      },
      "source": [
        "# scatter plot of results\n",
        "pyplot.scatter(cv_results, ideal_results)\n",
        "# plot the line of best fit\n",
        "coeff, bias = polyfit(cv_results, ideal_results, 1)\n",
        "line = coeff * asarray(cv_results) + bias\n",
        "pyplot.plot(cv_results, line, color='r')\n",
        "# label the plot\n",
        "pyplot.title('10-fold CV vs LOOCV Mean Accuracy')\n",
        "pyplot.xlabel('Mean Accuracy (10-fold CV)')\n",
        "pyplot.ylabel('Mean Accuracy (LOOCV)')\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inGOWJFmilDH"
      },
      "source": [
        "**Assignment**\n",
        "After viewing the plot, we are inclined to use the 10-fold CV - for this dataset and the model. Maybe we should use support vector machines for this dataset. \n",
        "\n",
        "Now, let's change the dataset. <br>\n",
        "- Change the number of samples\n",
        "- Change the number of features, both redundant and non reundant\n"
      ]
    }
  ]
}