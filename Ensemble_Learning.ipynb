{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPd2JR+P81fjn9ck8vttnfU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we learn:\n",
        "\n",
        "Weighted Average Ensembles are an extension to voting ensembles where model votes are proportional to model performance.\n",
        "How to develop weighted average ensembles using the voting ensemble from scikit-learn.\n",
        "\n",
        "How to evaluate the Weighted Average Ensembles for classification and regression and confirm the models are skillful."
      ],
      "metadata": {
        "id": "dkc7rHrSxnaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/Machine-Learning.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "C6LQs_oxxg91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "dFUplS0Yx9kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/images/ensemble.png\")"
      ],
      "metadata": {
        "id": "GMVpmMtFz5d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagged Decision Trees for Classification"
      ],
      "metadata": {
        "id": "96BodFEhu6nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning.\n",
        "\n",
        "In the example below see an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created."
      ],
      "metadata": {
        "id": "gODGefvRtznc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "UyE2a2mUyCGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "seed = 7"
      ],
      "metadata": {
        "id": "tLcrP5uzyHXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYqLXv3btK8T"
      },
      "outputs": [],
      "source": [
        "# Bagged Decision Trees for Classification\n",
        "\n",
        "kfold = model_selection.KFold(n_splits=10)\n",
        "cart = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "model = BaggingClassifier(estimator=cart, n_estimators=num_trees, random_state=seed)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forests"
      ],
      "metadata": {
        "id": "XxdBfnotu-cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest is an extension of bagged decision trees.\n",
        "\n",
        "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.\n",
        "\n",
        "You can construct a Random Forest model for classification using the RandomForestClassifier class.\n",
        "\n",
        "The example below provides an example of Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features."
      ],
      "metadata": {
        "id": "eXL3OrPrt7aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "rmf5DlYQyYcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classification\n",
        "\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "kfold = model_selection.KFold(n_splits=10)\n",
        "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "id": "2G-dEg7yt1xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Trees"
      ],
      "metadata": {
        "id": "41sTIcuevBNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.\n",
        "\n",
        "You can construct an Extra Trees model for classification using the ExtraTreesClassifier class.\n",
        "\n",
        "The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features."
      ],
      "metadata": {
        "id": "vRgM3rCguBTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier"
      ],
      "metadata": {
        "id": "H831oiYqyeho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra Trees Classification\n",
        "num_trees = 100\n",
        "max_features = 7\n",
        "kfold = model_selection.KFold(n_splits=10)\n",
        "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "id": "viS8UPHIuDmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting Algorithms"
      ],
      "metadata": {
        "id": "94a11-2yvFdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting Algorithms\n",
        "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n",
        "\n",
        "Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction.\n",
        "\n",
        "The two most common boosting ensemble machine learning algorithms are:\n",
        "\n",
        "AdaBoost\n",
        "Stochastic Gradient Boosting\n"
      ],
      "metadata": {
        "id": "gVtBDhNmuN9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost\n",
        "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or or less attention to them in the construction of subsequent models.\n",
        "\n",
        "You can construct an AdaBoost model for classification using the AdaBoostClassifier class.\n",
        "\n",
        "The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "AbJ7uP-CuRa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "metadata": {
        "id": "0HBL7eG2yjwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaBoost Classification\n",
        "num_trees = 30\n",
        "kfold = model_selection.KFold(n_splits=10)\n",
        "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "id": "EW6PyOrzuSPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Boosting\n",
        "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps of the the best techniques available for improving performance via ensembles.\n",
        "\n",
        "You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class.\n",
        "\n",
        "The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees."
      ],
      "metadata": {
        "id": "OxY2_MPyuY04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "a0IPwZnpypBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic Gradient Boosting Classification\n",
        "num_trees = 100\n",
        "kfold = model_selection.KFold(n_splits=10)\n",
        "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "metadata": {
        "id": "e9DPOw6tuZfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voting Ensemble\n",
        "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
        "\n",
        "It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
        "\n",
        "The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked generalization) and is currently not provided in scikit-learn.\n",
        "\n",
        "You can create a voting ensemble model for classification using the VotingClassifier class.\n",
        "\n",
        "The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem."
      ],
      "metadata": {
        "id": "OcA02FtMufWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/images/voting hard.png\")"
      ],
      "metadata": {
        "id": "P-wa5_gC0EHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/images/voting soft.png\")"
      ],
      "metadata": {
        "id": "X3txqbyk0JR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(\"/content/cloned-repo/images/vote with weights.png\")"
      ],
      "metadata": {
        "id": "0Hkg14xA0N2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate a weighted average ensemble for regression with rankings for model weights\n",
        "from numpy import argsort\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# get a list of base models\n",
        "def get_models():\n",
        " models = list()\n",
        " models.append(('knn', KNeighborsRegressor()))\n",
        " models.append(('cart', DecisionTreeRegressor()))\n",
        " models.append(('svm', SVR()))\n",
        " return models"
      ],
      "metadata": {
        "id": "XQ28n9c2yvYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate each base model\n",
        "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
        "\t# fit and evaluate the models\n",
        "\tscores = list()\n",
        "\tfor name, model in models:\n",
        "\t\t# fit the model\n",
        "\t\tmodel.fit(X_train, y_train)\n",
        "\t\t# evaluate the model\n",
        "\t\tyhat = model.predict(X_val)\n",
        "\t\tmae = mean_absolute_error(y_val, yhat)\n",
        "\t\t# store the performance\n",
        "\t\tscores.append(-mae)\n",
        "\t\t# report model performance\n",
        "\treturn scores"
      ],
      "metadata": {
        "id": "oCy2YvRVzUhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset\n",
        "X, y = make_regression(n_samples=10000, n_features=20, n_informative=10, noise=0.3, random_state=7)\n",
        "# split dataset into train and test sets\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
        "# split the full train set into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n"
      ],
      "metadata": {
        "id": "XAqdsoyxzeIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Voting Ensemble for Classification\n",
        "\n",
        "# create the base models\n",
        "models = get_models()\n",
        "# fit and evaluate each model\n",
        "scores = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
        "print(scores)\n",
        "ranking = 1 + argsort(argsort(scores))\n",
        "print(ranking)\n",
        "# create the ensemble\n",
        "ensemble = VotingRegressor(estimators=models, weights=ranking)\n",
        "# fit the ensemble on the training dataset\n",
        "ensemble.fit(X_train_full, y_train_full)\n",
        "# make predictions on test set\n",
        "yhat = ensemble.predict(X_test)\n",
        "# evaluate predictions\n",
        "score = mean_absolute_error(y_test, yhat)\n",
        "print('Weighted Avg MAE: %.3f' % (score))\n",
        "# evaluate each standalone model\n",
        "scores = evaluate_models(models, X_train_full, X_test, y_train_full, y_test)\n",
        "for i in range(len(models)):\n",
        " print('>%s: %.3f' % (models[i][0], scores[i]))\n",
        "# evaluate equal weighting\n",
        "ensemble = VotingRegressor(estimators=models)\n",
        "ensemble.fit(X_train_full, y_train_full)\n",
        "yhat = ensemble.predict(X_test)\n",
        "score = mean_absolute_error(y_test, yhat)\n",
        "print('Voting MAE: %.3f' % (score))"
      ],
      "metadata": {
        "id": "8gj14Arlujm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}