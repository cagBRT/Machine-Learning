{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wage_per_hour.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOg9ezHua3Fd4BLBA0M1Yf6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/HP2%20Wage_per_hour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SQKWd5CyYaO"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/Machine-Learning.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4doZ6C0zjUH"
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qp8aXDwUM_z"
      },
      "source": [
        "# **Can a person's hourly wage be predicted from a set of features?**\n",
        "\n",
        "This model uses linear regression:<br>\n",
        ">Multiple inputs<br>\n",
        ">One float output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHyhWXYgzmVP"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvYo7B7xqch"
      },
      "source": [
        "#read in data using pandas\n",
        "dataset = pd.read_csv(\"hourly_wages.csv\")\n",
        "#check data has been read in properly\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKxgI6aJVU50"
      },
      "source": [
        "wage_per_hour --> the label<br>\n",
        "all other columns --> features<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VeAAWJaUo76"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13h97-mb0D2w"
      },
      "source": [
        "dataset.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2CpVHrSeMfs"
      },
      "source": [
        "dataset[\"female\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7YSOmeIlSZl"
      },
      "source": [
        "dataset[\"age\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl2sDHdZaenT"
      },
      "source": [
        "corr = dataset.corr()\n",
        "sns.heatmap(corr, \n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSpXOewJmvTc"
      },
      "source": [
        "corr.columns.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRjQcCAdVBK2"
      },
      "source": [
        "# **Split the dataset into training and test sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPqzRxeVHSG"
      },
      "source": [
        "This is a hyperparameter that can be adjusted. <br>\n",
        "0.95 - 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_uOMdheypB2"
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1PvLzIVVg1a"
      },
      "source": [
        "# **Separate the label from the features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTigEatR0bSJ"
      },
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop(\"wage_per_hour\")\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waQ318910hxx"
      },
      "source": [
        "test_stats = test_dataset.describe()\n",
        "test_stats.pop(\"wage_per_hour\")\n",
        "test_stats = test_stats.transpose()\n",
        "test_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sOILWDA0mff"
      },
      "source": [
        "train_labels = train_dataset.pop('wage_per_hour')\n",
        "test_labels = test_dataset.pop('wage_per_hour')\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkB5m7bKWV15"
      },
      "source": [
        "# **Normalize the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1QKRKK0swD"
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFaQ8d8YWZei"
      },
      "source": [
        "# **Create the model**\n",
        "Tune the hyperparameters to improve the model performance. <br>\n",
        "1. Number of nodes in each layers\n",
        "2. Number and kinds of layers\n",
        "3. activation functions\n",
        "4. The learning rate in the optimizer (.0001 - .1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jssbPzTdyxQD"
      },
      "source": [
        "inputs = len(train_dataset.keys())\n",
        "print(\"number of inputs to the model = \" + str(inputs))\n",
        "\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(8, activation=tf.nn.relu,input_shape=([len(train_dataset.keys())]),),\n",
        "    #layers.Dropout(0.2),\n",
        "\n",
        "    #layers.Dense(8, activation=tf.nn.relu),\n",
        "    #layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(8, activation=tf.nn.relu),\n",
        "    #layers.Dropout(0.2),\n",
        "    \n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "  return model\n",
        "  print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGUY-cQFy2ll"
      },
      "source": [
        "model = build_model()\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ2o0NssXrYV"
      },
      "source": [
        "# **Train the Model**\n",
        "Modify this hyperparameter<br>\n",
        "1. Number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23k2bVx51WWN"
      },
      "source": [
        "# Display training progress by printing a single dot for each completed epoch\n",
        "\n",
        "model = build_model()\n",
        "EPOCHS = 1000\n",
        "\n",
        "# The patience parameter is the amount of epochs to check for improvement\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "history = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
        "                    validation_split = 0.2, verbose=0, callbacks=[early_stop])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zd4JcKv1Ps1"
      },
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error [wage per hour]')\n",
        "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
        "           label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
        "           label = 'Val Error')\n",
        "  plt.ylim([0,5])\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsKJDp5d20o-"
      },
      "source": [
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=1)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: ${:5.2f} wage_per_hour\".format(mae))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8FsVvMyqUvb"
      },
      "source": [
        "\n",
        "Discretization, also known as quantization or binning, divides a continuous feature into a pre-specified number of categories (bins), and thus makes the data discrete.<br>\n",
        "One of the main goals of a discretization is to significantly reduce the number of discrete intervals of a continuous attribute. Hence, why this transformation can increase the performance of tree based models.<br>\n",
        "Sklearn provides a KBinsDiscretizer class that can take care of this.<br>\n",
        " The only thing you have to specify are:<br> the number of bins (n_bins) for each feature and how to encode these bins (ordinal, onehot or onehot-dense).<br>\n",
        "\n",
        "The optional strategy parameter can be set to three values:<br>\n",
        ">**uniform**, where all bins in each feature have identical widths.<br>\n",
        "**quantile** (default), where all bins in each feature have the same number of points.<br>\n",
        "**kmeans**, where all values in each bin have the same nearest center of a 1D k-means cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukb-7c0PqT_-"
      },
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "disc = KBinsDiscretizer(n_bins=3, encode='onehot', \n",
        "                        strategy='uniform')\n",
        "disc.fit_transform(normed_train_data) #X is the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrQ8Hk2nqfHd"
      },
      "source": [
        "If the output doesn’t make sense to you, invoke the bin_edges_ attribute on the discretizer (disc) and take a look at how the bins are divided. Then try another strategy and see how the bin edges change accordingly."
      ]
    }
  ]
}