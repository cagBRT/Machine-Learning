{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrossValidation1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/CrossValidation1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG_bC5qLDIdI"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -s https://github.com/cagBRT/Machine-Learning.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skeDVtEBtb25"
      },
      "source": [
        "# **Intro to Cross Validation (CV)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fcm9DObt2_5"
      },
      "source": [
        "When training a model the first thing we do is:\n",
        ">divide the data into training and test sets\n",
        "\n",
        "The model is trained on the training set. <br>\n",
        "Then to test the accuracy of the model predictions, it is tested using data it has never seen before... the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JVnPGpRuRB4"
      },
      "source": [
        "Once we have a model that is close to our performance criteria, we can do hyperparameter tuning to select the best hyperparameters for the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMUFfx4rug2i"
      },
      "source": [
        "The problem with this approach is:\n",
        ">We may actually now fit the hyperparameters to the test set. <br>\n",
        "**Leading to a risk of overfitting on the test set** because the parameters can be tweaked to get optimal model performance. <br>\n",
        "\n",
        "A common method to solve this problem is to divide the test set into test and validate sets. Then train on the training set, test on the test set, and when ready, validate performance on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm7OqaoUtyHw"
      },
      "source": [
        "The downsides of this method are: \n",
        "1. Reducing the amount of data available for training the model. \n",
        "2. The results of the model performance are variable depending upon the random selection of the training and test sets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlqzutJhwEqz"
      },
      "source": [
        "This problem can be solved by using **Cross-validation(CV)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rlpaSm7wPpE"
      },
      "source": [
        "If we use Cross-validation, a validation set is no longer necessary. We need only the training and the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhM3yzY0wshU"
      },
      "source": [
        "There are several cross-validation methods. <br>\n",
        "We will look at the basic methods.<br>\n",
        "You can do a deeper dive into CV by<br>\n",
        "checking this [link](https://scikit-learn.org/stable/modules/cross_validation.html) <br>\n",
        "Basic Cross Validation Methods we will explore in these two notebooks: <br>\n",
        "- Holdout\n",
        "- K-fold CV\n",
        "- Leave One Out (LOOCV)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HptFcgwQi9n"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWw4hsd47qx"
      },
      "source": [
        "# **Holdout**\n",
        "This is the basic method for testing models. <br>\n",
        "1. Split the dataset into training and test sets.<br>\n",
        "2. Train the model on the training set<br>\n",
        "3. Test the model on the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3MbOCS3AnlP"
      },
      "source": [
        "**Create synthetic data**<br>\n",
        "The data set we are creating has 1000 samples and three clusters<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9D9xB-OKdRj"
      },
      "source": [
        "# demonstrate that the train-test split procedure is repeatable\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "# create dataset\n",
        "X, y = make_blobs(n_samples=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6cikBZkfyMk"
      },
      "source": [
        "**Train-test split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvsaFREAuSV"
      },
      "source": [
        "Split the dataset into training and test sets. Use 80% of the data for training. \n",
        "20% for testing<br>\n",
        "Note the accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjhkqTqzJdGi"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(X.shape, y.shape)\n",
        "# split into train test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \n",
        "                                                    random_state=1)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# fit the model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "# make predictions\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1FGeVHlO3Tn"
      },
      "source": [
        "Using the same data, do another 80/20 split of the data<br>\n",
        "Note the accuracy the model this time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxN5sJx8LKFS"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# fit the model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "# make predictions\n",
        "yhat = model.predict(X_test)\n",
        "# evaluate predictions\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7U9Cb9nPKFj"
      },
      "source": [
        "The accuracy changes depending upon the train-test split. Even when the split is the same percentage. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JZGP-dyPjIm"
      },
      "source": [
        "The issue of the changing accuracy depending up the dataset split can be solved using different Cross validation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWOvHjPITK1P"
      },
      "source": [
        "# **K-Fold Cross Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QovXSDhpREM8"
      },
      "source": [
        "# **K-fold CV**<br>\n",
        "K-fold CV uses the following algorithm: \n",
        "1. Randomly split the data into K equally sized parts\n",
        "2. Select a single partition from the K parts to be the test set\n",
        "3. Train the model K times, once for each K\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zrDQ-JGt9Ca"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(\"images/grid_search_cross_validation.png\" , width=640)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxpUmFQKTs8W"
      },
      "source": [
        "KFold divides all the samples in  groups of samples, called folds, of equal sizes (if possible).<br>\n",
        " The prediction function is learned using  folds, and the fold left out is used for test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7yBXyhBB4fd"
      },
      "source": [
        "The KFold general procedure:\n",
        "\n",
        "- Shuffle the dataset randomly.\n",
        "- Split the dataset into k groups\n",
        "- For each unique group:\n",
        ">Take the group as a hold out or test data set<br>\n",
        ">Take the remaining groups as a training data set<br>\n",
        ">Fit a model on the training set and evaluate it on the test set<br>\n",
        ">Retain the evaluation score and discard the model<br>\n",
        "\n",
        "Summarize the skill of the model using the sample of model evaluation scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5FoJ9eiVJKk"
      },
      "source": [
        "The k value must be chosen carefully for your data sample.<br>\n",
        "\n",
        "A poorly chosen value for k may result in a mis-representative idea of the skill of the model, such as a score with a high variance (that may change a lot based on the data used to fit the model), or a high bias, (such as an overestimate of the skill of the model).<br>\n",
        "\n",
        "Three common tactics for choosing a value for k are as follows:<br>\n",
        "\n",
        ">- Representative: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.<br>\n",
        "- k=10: The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.<br>\n",
        "- k=n: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzrCESVEVkK0"
      },
      "source": [
        "Three models are trained and evaluated with each fold given a chance to be the held out test set.<br>\n",
        "\n",
        "For example:<br>\n",
        "\n",
        "Model1: Trained on Fold1 + Fold2, Tested on Fold3<br>\n",
        "Model2: Trained on Fold2 + Fold3, Tested on Fold1<br>\n",
        "Model3: Trained on Fold1 + Fold3, Tested on Fold2<br>\n",
        "The models are then discarded after they are evaluated as they have served their purpose.<br>\n",
        "\n",
        "The skill scores are collected for each model and summarized for use.<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyWuDvxkB3Or"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
        "y = np.array([1, 2, 3, 4])\n",
        "kf = KFold(n_splits=2)\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "print(kf)\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM4s1KlzB-5z"
      },
      "source": [
        "Setup the K-fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnQbqnoYV2BR"
      },
      "source": [
        "dataIn=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1keIY1qsWA3Y"
      },
      "source": [
        "kfold = KFold(3, True, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIugjEd2B-Dx"
      },
      "source": [
        "# enumerate splits\n",
        "for train, test in kfold.split(dataIn):\n",
        "\tprint('train: %s, test: %s' % (train, test))\n",
        " \n",
        "#numbers are the index of the element"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzVgFXQxWAaR"
      },
      "source": [
        "# scikit-learn k-fold cross-validation\n",
        "from numpy import array\n",
        "from sklearn.model_selection import KFold\n",
        "# data sample\n",
        "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
        "# prepare cross validation\n",
        "kfold = KFold(3, True, 1)\n",
        "# enumerate splits\n",
        "for train, test in kfold.split(data):\n",
        "\tprint('train: %s, test: %s' % (data[train], data[test]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd4D30jaW9UW"
      },
      "source": [
        "Larger KFold dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qsjl_sDW9d2"
      },
      "source": [
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=100, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td4iYPIRXENO"
      },
      "source": [
        "# evaluate a logistic regression model using k-fold cross-validation\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# create dataset\n",
        "X, y = make_classification(n_samples=100, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "# create model\n",
        "model = LogisticRegression()\n",
        "# evaluate model\n",
        "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "for i in range(len(scores)):\n",
        "  print(\"Test#:\",i,\"=\",scores[i],\"\\n\")\n",
        "# report performance\n",
        "print('Average of scores is Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI-qM3d3X0SQ"
      },
      "source": [
        "**Assignment**<br>\n",
        "Increase the number of data samples to 1000<br>\n",
        "Check the accuracy average of different k values (try 2-15)<br>\n",
        "Report to the class your findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf2g3DAxYRiI"
      },
      "source": [
        "The expectation is:<br>\n",
        "low values of k will result in a noisy estimate of model performance <br>\n",
        "large values of k will result in a better estimate of model performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bOjR0daY5jj"
      },
      "source": [
        "# **Leave One Out (LOOCV)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22nD2K3nY_ps"
      },
      "source": [
        "To calculate the best 'k', we would train the model on all available data and estimate the performance on a separate large and representative hold-out dataset.<br>\n",
        "The performance on this hold-out dataset would represent the “true” performance of the model and any cross-validation performances on the training dataset would represent an estimate of this score.<br>\n",
        "\n",
        "The KFold method is good, if you have a large enough dataset to leave out a large percentage of your dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nutz1CR2Y924"
      },
      "source": [
        "If you do not have a large dataset, you can use Leave One Out CV. It is a computationally expensive version of cross-validation where k=N, and N is the total number of examples in the training dataset. <br>\n",
        "\n",
        "Each sample in the training set is given an example to be used alone as the test evaluation dataset. It is rarely used for large datasets because of the compute expense, but it provides a good estimate of model performance given the available data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsetYYKWbAPM"
      },
      "source": [
        "from numpy import mean\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzUsh1wfa520"
      },
      "source": [
        "# create the dataset\n",
        "#Change the dataset size here:\n",
        "def get_dataset(n_samples=100):\n",
        "\tX, y = make_classification(n_samples=n_samples, n_features=20, \n",
        "                            n_informative=15, n_redundant=5, \n",
        "                            random_state=1)\n",
        "\treturn X, y\n",
        "\n",
        "# retrieve the model to be evaluate\n",
        "def get_model():\n",
        "\tmodel = LogisticRegression()\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe8QvEWQbksa"
      },
      "source": [
        "# evaluate the model using a given test condition\n",
        "def evaluate_model(cv):\n",
        "\t# get the dataset\n",
        "\tX, y = get_dataset()\n",
        "\t# get the model\n",
        "\tmodel = get_model()\n",
        "\t# evaluate the model\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\t# return scores\n",
        "\treturn mean(scores), scores.min(), scores.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_lnjNsyciwU"
      },
      "source": [
        "Running the example first reports the LOOCV, <br>\n",
        "then the mean, min, and max accuracy for each k value that was evaluated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSiNXNJ_cUsD"
      },
      "source": [
        "# calculate the ideal test condition\n",
        "ideal, _, _ = evaluate_model(LeaveOneOut())\n",
        "print('Ideal: %.3f' % ideal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV9HgExhbL1o"
      },
      "source": [
        "# define folds to test\n",
        "folds = range(2,31)\n",
        "# record mean and min/max of each set of results\n",
        "means, mins, maxs = list(),list(),list()\n",
        "# evaluate each k value\n",
        "for k in folds:\n",
        "\t# define the test condition\n",
        "\tcv = KFold(n_splits=k, shuffle=True, random_state=1)\n",
        "\t# evaluate k value\n",
        "\tk_mean, k_min, k_max = evaluate_model(cv)\n",
        "\t# report performance\n",
        "\tprint('> folds=%d, accuracy=%.3f (%.3f,%.3f)' % (k, k_mean, k_min, k_max))\n",
        "\t# store mean accuracy\n",
        "\tmeans.append(k_mean)\n",
        "\t# store min and max relative to the mean\n",
        "\tmins.append(k_mean - k_min)\n",
        "\tmaxs.append(k_max - k_mean)\n",
        "# line plot of k mean values with min/max error bars\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpiYIXMwdIkZ"
      },
      "source": [
        "In the plot below the red line is the ideal value of accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8ULxhmqcuuU"
      },
      "source": [
        "pyplot.errorbar(folds, means, yerr=[mins, maxs], fmt='o')\n",
        "# plot the ideal case in a separate color\n",
        "pyplot.plot(folds, [ideal for _ in range(len(folds))], color='r')\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq3ViO86c7DV"
      },
      "source": [
        "The plot for this model on this dataset, **most k values underestimate the performance of the model compared to the ideal case**. <br>\n",
        "The results suggest that perhaps k=10 alone is slightly optimistic and perhaps k=13 might be a more accurate estimate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AIALQObdteA"
      },
      "source": [
        "**Assignment**<br>\n",
        "Change the size of the dataset to a larger number. <br>\n",
        "Rerun the LOOCV. <br>\n",
        "Report to the class what happened<br>"
      ]
    }
  ]
}