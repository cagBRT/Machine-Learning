{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayesGaussian 1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOrCaXBVcBGFWMd75r+3eZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Machine-Learning/blob/master/NaiveBayesGaussian_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afx-5KSDwPa_"
      },
      "source": [
        "**Naïve Bayes** algorithms is a classification technique based on applying Bayes’ theorem with a strong assumption that all the predictors are independent to each other. <br>\n",
        "In other words, the assumption is that the presence of a feature in a class is independent to the presence of any other feature in the same class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MjE6BQwwVRQ"
      },
      "source": [
        "For example:<br> \n",
        ">a phone may be considered as smart if it has a touch screen, internet, a camera etc. Although these features are dependent on each other, they contribute *independently* to the probability of that the phone is a smart phone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCpud4OHwx4I"
      },
      "source": [
        "**Pros**<br>\n",
        "The followings are some pros of using Naïve Bayes classifiers <br>\n",
        "\n",
        "- Naïve Bayes classification is easy to implement and fast.<br>\n",
        "\n",
        "- It converges faster than models like logistic regression.<br>\n",
        "\n",
        "- It requires less training data.<br>\n",
        "\n",
        "- It is highly scalable in nature. They scale linearly with the number of predictors and data points.<br>\n",
        "\n",
        "- It can make probabilistic predictions and can handle continuous as well as discrete data.<br>\n",
        "\n",
        "- Naïve Bayes classification algorithm can be used for binary as well as multi-class classification problems both.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3GDfR5nwz62"
      },
      "source": [
        "**Cons:**<br>\n",
        "The followings are some cons of using Naïve Bayes classifiers <br>\n",
        "\n",
        "- One of the most important cons of Naïve Bayes classification is its strong feature independence because *in real life it is almost impossible to have a set of features which are completely independent of each other*.<br>\n",
        "\n",
        "- ‘Zero frequency’ which means that if a categorial variable has a category that is not observed in training data set, then the Naïve Bayes model will assign a zero probability to it and it will be unable to make a prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdlgzahGxqKj"
      },
      "source": [
        "The following are some common applications of Naïve Bayes classification<br>\n",
        "\n",
        "- Real-time prediction − Its ease of implementation and fast computation means it can be used to do prediction in real-time.\n",
        "\n",
        "- Multi-class prediction − Naïve Bayes classification algorithm can be used to predict the probability of multiple classes of target variable.\n",
        "\n",
        "- Text classification − Due to the feature of multi-class prediction, Naïve Bayes classification algorithms are well suited for text classification. It is used for spam-filtering and sentiment analysis.\n",
        "\n",
        "- Recommendation system − Along with the algorithms like collaborative filtering, Naïve Bayes can be used as a Recommendation system which can filter unseen information and predicts whether a user would like a given suggestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcwPGusFwLXY"
      },
      "source": [
        "Gaussian Naïve Bayes\n",
        "It **is the simplest Naïve Bayes classifier** having the assumption that the data from each label is drawn from a simple Gaussian distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCBaqWODMm-x"
      },
      "source": [
        "https://machinelearningmastery.com/naive-bayes-for-machine-learning/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPsAufZyB2ew"
      },
      "source": [
        "Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Uf0WxFnazI"
      },
      "source": [
        "#!pip install -U matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1ChqDsEg6hN"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import matplotlib.pyplot as plty\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "#!pip uninstall matplotlib\n",
        "#y\n",
        "!pip install matplotlib==3.1.3\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsELXrLdB6gi"
      },
      "source": [
        "Create a data set and plot it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijf9m51NQDBr"
      },
      "source": [
        "**make_blobs and make_classification** create multiclass datasets by allocating each class one or more normally-distributed clusters of points.<br><br>\n",
        "\n",
        "make_blobs provides:<br>\n",
        "- greater control regarding the centers and standard deviations of each cluster<br>\n",
        "- is used to demonstrate clustering.<br><br>\n",
        "\n",
        "make_classification specialises in introducing noise by way of:<br>\n",
        "- correlated, redundant and uninformative features; <br>- multiple Gaussian clusters per class<br>\n",
        "- linear transformations of the feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ov7KPQsNA6_"
      },
      "source": [
        "X,y = make_classification(n_samples=100, n_features=10,n_classes=3,class_sep=1.0,n_clusters_per_class=1)\n",
        "#X, y = make_blobs(2000, 2, centers=2, random_state=2, cluster_std=1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NUdEkLTigSJ"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-CN-iw8kf-X"
      },
      "source": [
        "# splitting X and y into training and testing sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl7wIq3uRCww"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "fig = plt.scatter(X_train[:,0],y_train, marker='o', c=X_train[:,1],s=25, cmap='RdBu')\n",
        "plt.xlabel(\"X_train[:,0]\")\n",
        "plt.ylabel(\"y_train\")\n",
        "plt.colorbar(fig)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8QuyNBvDR_m"
      },
      "source": [
        "Create and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD5k81y7KNoW"
      },
      "source": [
        "model = GaussianNB()\n",
        "model.fit(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncgc83gHhDaU"
      },
      "source": [
        "ypred = model.predict_proba(X_test)\n",
        "ypredClass = ypred.round()\n",
        "#print(ypredClass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HecblgCrLk62"
      },
      "source": [
        "model.score(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zoe_uHqZMjDX"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "fig = plt.scatter(X_train[:,0],y_train, marker='o', c=X_train[:,1],s=25, cmap='RdBu')\n",
        "plt.xlabel(\"X_train[:,0]\")\n",
        "plt.ylabel(\"y_train\")\n",
        "plt.colorbar(fig)\n",
        "plt.plot(X_test[:,0],ypredClass,'r+')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is0meUzlMuc-"
      },
      "source": [
        "Assignment<br>\n",
        "Change the dataset:<br>\n",
        ">fewer data instances<br>\n",
        ">more data instances<br>\n",
        ">number of clusters<br>"
      ]
    }
  ]
}